{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35bc537",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO, BytesIO\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7df31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter Layer\n",
    "\n",
    "def read_csv_to_df(bucket, key, decoding = 'utf-8', sep = ','):\n",
    "    csv_obj = bucket.Object(key=key).get().get('Body').read().decode(decoding)\n",
    "    data = StringIO(csv_obj)\n",
    "    df = pd.read_csv(data, delimiter = sep)\n",
    "    return df\n",
    "\n",
    "def write_df_to_s3(bucket, df, key):\n",
    "    out_buffer = BytesIO()\n",
    "    df.to_parquet(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def write_df_to_s3_csv(bucket, df, key):\n",
    "    out_buffer = StringIO()\n",
    "    df.to_csv(out_buffer, index=False)\n",
    "    bucket.put_object(Body=out_buffer.getvalue(), Key=key)\n",
    "    return True\n",
    "\n",
    "def list_files_in_prefix(bucket, prefix):\n",
    "    files = [obj.key for obj in bucket.objects.filter(Prefix = prefix)]\n",
    "    return files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc1c9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application layer - not core\n",
    "\n",
    "def return_date_list(bucket, arg_date, src_format, meta_key):\n",
    "    min_date = datetime.strptime(arg_date,src_format).date() - timedelta(days=1)\n",
    "    today = datetime.today().date()\n",
    "    try:\n",
    "        df_meta = read_csv_to_df(bucket, meta_key)\n",
    "        print(df_meta)        \n",
    "        dates = [(min_date + timedelta(days = x)) for x in range(0,(today - min_date).days + 1)]\n",
    "        src_dates = set(pd.to_datetime(df_meta['source_date']).dt.date)\n",
    "        dates_missing = set(dates[1:]) - src_dates\n",
    "        if dates_missing:\n",
    "            min_date  = min(set(dates[1:]) - src_dates) - timedelta(days=1)\n",
    "            return_dates  = [date.strftime(src_format) for date in dates if date >= min_date]\n",
    "            return_min_date = (min_date + timedelta(days=1)).strftime(src_format)\n",
    "        else:\n",
    "            return_dates = []\n",
    "            return_min_date = datetime(2200,1,1).date()\n",
    "    except bucket.session.client('s3').exceptions.NoSuchKey:\n",
    "        return_dates = [(min_date + timedelta(days = x)).strftime(src_format) for x in range(0,(today - min_date).days + 1)]\n",
    "        return_min_date = arg_date\n",
    "    print(return_min_date, return_dates)\n",
    "    return return_min_date, return_dates\n",
    "\n",
    "def update_meta_file(bucket, meta_key, extract_date_list):\n",
    "    df_new = pd.DataFrame(columns=['source_date', 'datetime_of_processing'])\n",
    "    df_new['source_date'] = extract_date_list\n",
    "    df_new['datetime_of_processing'] = datetime.today().strftime('%Y-%m-%d')\n",
    "    df_old = read_csv_to_df(bucket, meta_key)\n",
    "    df_all = pd.concat([df_old, df_new])\n",
    "    print(df_all)\n",
    "    write_df_to_s3_csv(bucket, df_all, meta_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f008cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application Layer\n",
    "\n",
    "def extract(bucket, date_list):\n",
    "    files = [key for date in date_list for key in list_files_in_prefix(bucket, date)]\n",
    "    df = pd.concat([read_csv_to_df(bucket, obj) for obj in files], ignore_index = True)\n",
    "    return df\n",
    "\n",
    "def transform_report1(df, columns, arg_date):\n",
    "    df = df.loc[:,columns]\n",
    "    df.dropna(inplace=True)\n",
    "    df['opening_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('first')\n",
    "    df['closing_price'] = df.sort_values(by=['Time']).groupby(['ISIN', 'Date'])['StartPrice'].transform('last')\n",
    "    df= df.groupby(['ISIN', 'Date'], as_index = False)\\\n",
    "    .agg(opening_price_eur = ('opening_price', 'min'),\n",
    "     closing_price_eur = ('closing_price', 'min'),\n",
    "     minimum_price_eur = ('MinPrice', 'min'),\n",
    "     maximum_price_eur = ('MaxPrice', 'max'),\n",
    "    daily_traded_volume = ('TradedVolume', 'sum'))\n",
    "    df['prev_closing_price'] = df.sort_values(by='Date').groupby(['ISIN'])['closing_price_eur'].shift(1)\n",
    "    df['change_prev_closing_%'] = (df['closing_price_eur'] - df['prev_closing_price']) / df['prev_closing_price'] * 100\n",
    "    df.drop(columns=['prev_closing_price'], inplace= True)\n",
    "    df = df.round(decimals=2)\n",
    "    df = df[df.Date >= arg_date]\n",
    "    return df\n",
    "\n",
    "def load(bucket, df, trg_key, trg_format, meta_key, extract_date_list):\n",
    "    key = trg_key + datetime.today().strftime('%Y%m%d_%H%M%S') + trg_format\n",
    "    write_df_to_s3(bucket, df, key)\n",
    "    update_meta_file(bucket, meta_key, extract_date_list)\n",
    "    return True\n",
    "\n",
    "def etl_report1(src_bucket, trg_bucket, date_list, columns, arg_date, trg_key, trg_format, meta_key):\n",
    "    df = extract(src_bucket, date_list)\n",
    "    df = transform_report1(df, columns, arg_date)\n",
    "    extract_date_list = [date for date in date_list if date >= arg_date]\n",
    "    load(trg_bucket, df, trg_key, trg_format, meta_key, extract_date_list)\n",
    "    return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "513e977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main function entrypoint\n",
    "def main():\n",
    "    # parameters\n",
    "    # Later read config\n",
    "    arg_date = '2022-11-05'\n",
    "    src_format = '%Y-%m-%d'\n",
    "    src_bucket = 'xetra-1234'\n",
    "    trg_bucket = 'test-xetra'\n",
    "    columns = ['ISIN',  'Date', 'Time', 'StartPrice', 'MaxPrice', 'MinPrice',\n",
    "       'EndPrice', 'TradedVolume']\n",
    "    trg_key = 'xetra_daily_report_'\n",
    "    trg_format = '.parquet'\n",
    "    meta_key='meta_file.csv'\n",
    "    \n",
    "    # Init\n",
    "    s3 = boto3.resource('s3')\n",
    "    bucket_src = s3.Bucket(src_bucket)\n",
    "    bucket_trg = s3.Bucket(trg_bucket)\n",
    "    \n",
    "    #run application\n",
    "    extract_date, date_list = return_date_list(bucket_trg, arg_date, src_format, meta_key)\n",
    "    etl_report1(bucket_src, bucket_trg, date_list, columns, extract_date, trg_key, trg_format, meta_key)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86bce7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  source_date datetime_of_processing\n",
      "0  2022-10-25    2022-10-25 12:33:23\n",
      "1  2022-10-26    2022-10-26 12:30:21\n",
      "2022-11-05 ['2022-11-04', '2022-11-05', '2022-11-06', '2022-11-07', '2022-11-08']\n",
      "  source_date datetime_of_processing\n",
      "0  2022-10-25    2022-10-25 12:33:23\n",
      "1  2022-10-26    2022-10-26 12:30:21\n",
      "0  2022-11-05             2022-11-08\n",
      "1  2022-11-06             2022-11-08\n",
      "2  2022-11-07             2022-11-08\n",
      "3  2022-11-08             2022-11-08\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3874f92",
   "metadata": {},
   "source": [
    "## Reading the uploaded file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11a0013d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchBucket",
     "evalue": "An error occurred (NoSuchBucket) when calling the ListObjects operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m s3 \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mresource(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m bucket_trg \u001b[38;5;241m=\u001b[39m s3\u001b[38;5;241m.\u001b[39mBucket(trg_bucket)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m bucket_trg\u001b[38;5;241m.\u001b[39mobjects\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(obj\u001b[38;5;241m.\u001b[39mkey)\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\boto3\\resources\\collection.py:81\u001b[0m, in \u001b[0;36mResourceCollection.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     78\u001b[0m limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlimit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     80\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpages():\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m page:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m item\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\boto3\\resources\\collection.py:171\u001b[0m, in \u001b[0;36mResourceCollection.pages\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Now that we have a page iterator or single page of results\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;66;03m# we start processing and yielding individual items.\u001b[39;00m\n\u001b[0;32m    170\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pages:\n\u001b[0;32m    172\u001b[0m     page_items \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent, params, page):\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\botocore\\paginate.py:269\u001b[0m, in \u001b[0;36mPageIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inject_starting_params(current_kwargs)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    270\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extract_parsed_response(response)\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_request:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;66;03m# The first request is handled differently.  We could\u001b[39;00m\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;66;03m# possibly have a resume/starting token that tells us where\u001b[39;00m\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;66;03m# to index into the retrieved page.\u001b[39;00m\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\botocore\\paginate.py:357\u001b[0m, in \u001b[0;36mPageIterator._make_request\u001b[1;34m(self, current_kwargs)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, current_kwargs):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcurrent_kwargs)\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\botocore\\client.py:507\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    504\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    505\u001b[0m     )\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\xetra_1234-w_2pi4Ol\\lib\\site-packages\\botocore\\client.py:943\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m    941\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m parsed_response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    942\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m--> 943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    945\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the ListObjects operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "# Init\n",
    "trg_bucket = 'xetra-xetra-1234'\n",
    "s3 = boto3.resource('s3')\n",
    "bucket_trg = s3.Bucket(trg_bucket)\n",
    "\n",
    "\n",
    "for obj in bucket_trg.objects.all():\n",
    "    print(obj.key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7deb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prq_obj = bucket_trg.Object(key='xetra_daily_report_20221028_160645.parquet').get().get('Body').read()\n",
    "data = BytesIO(prq_obj)\n",
    "df_report = pd.read_parquet(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fdfcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b840fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3053a03f53b1c4bc95225bb23ec08d0976c426473438f8e7e3fad852fbe0fa77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
